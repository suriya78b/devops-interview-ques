# üê≥ Kubernetes (AKS/EKS) & Container Security
## 1. Write manifest YAML for different kind like service, secrets, volums, pod, deployment, etc.,
## 2. How do you upgrade your k8s cluster?
In a production environment, upgrading a Kubernetes cluster requires careful planning to ensure minimal disruption, high availability, and compatibility.

### First, planning phase:

1. Review the official Kubernetes release notes, changelog, and deprecation notices for the target version to identify breaking changes, removed APIs, or new features.
2. Check for deprecated/removed APIs in your manifests using tools like pluto or kube-no-trouble.
3. Backup etcd (critical for state) and ensure you have recent snapshots.
4. Test the upgrade in a staging environment that mirrors production (same add-ons, workloads, etc.).
5. Verify compatibility of add-ons (e.g., CSI drivers, ingress controllers, monitoring) and third-party tools.

The actual upgrade process depends on whether it's a self-managed cluster (e.g., using kubeadm) or managed (e.g., EKS, GKE, AKS).
### For self-managed (kubeadm) clusters:

1. On the primary control plane node: Run kubeadm upgrade plan to check feasibility and available versions.
2. Upgrade kubeadm: apt update && apt install kubeadm=<target-version>.
3. Apply the upgrade: kubeadm upgrade apply <target-version>.
4. Upgrade kubelet and kubectl on the control plane: apt install kubelet=<target-version> kubectl=<target-version> and restart kubelet.
4. Drain and upgrade additional control plane nodes one by one.
#### For worker nodes: 
- Drain the node (kubectl drain), upgrade kubeadm/kubelet, then kubeadm upgrade node, restart kubelet, and uncordon.
- Post-upgrade: Verify with kubectl get nodes, check pod status, and monitor for issues.

### For managed clusters (common in enterprises):

- AWS EKS: Use the AWS console/CLI (aws eks update-cluster-version) to upgrade the control plane (AWS handles it with zero downtime). Then update managed node groups (via console or eksctl) or self-managed nodes. EKS supports up to 3 minor version skew in newer versions.
- Google GKE: Enable auto-upgrades or manually upgrade via console/gcloud. Options include surge (add nodes during upgrade) or blue-green. GKE handles node auto-repairs and upgrades seamlessly.
- Azure AKS: Use Azure CLI/portal to upgrade control plane, then node pools. Supports planned maintenance windows.

### Best practices I follow:

- Use Pod Disruption Budgets (PDBs) and proper affinities to maintain availability during node drains.
- Upgrade one minor version at a time to reduce risk.
- Monitor during/after with tools like Prometheus or cloud-native monitoring.
- Have a rollback plan (e.g., etcd restore for self-managed; managed services often auto-rollback failed control plane upgrades).
- Schedule during maintenance windows and communicate with stakeholders.

## 6. RBAC
### RBAC: Role Based Access Control
> Securiy machanism that controls who can access the cluster and perform which actions. 
### RBAC:
RBAC is a method of regulating access to resources based on roles of individual users or service accounts. It uses roles and role bndings to define permissions

- Roles: Define set of permissions (verbs like get, list, watch, create, update, patch, delete, etc.) on specifc resources (like pod, deployemnts) with a namespace.
- RoleBindings: Assign those roles to users, groups, or service accounts
- ClusterRoles: Similar to Roles but apply it on cluster level (e.g., list nodes)
- ClusterRoleBindings: Bind ClusterRoles to subjects at the cluster level.
### Example 1: Developer read-only access in a namespace
Suppose we have a namespace dev and want a developer alice@example.com to only view pods and deployments (no create/delete).
```
# Role: pod-reader in namespace dev
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch"]
```
```
# RoleBinding: Bind the role to user alice
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pod-reader-binding
  namespace: dev
subjects:
- kind: User
  name: alice@example.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```
Now, Alice can run kubectl get pods -n dev but not kubectl delete pod.
### Example 2: Service account for a monitoring pod
A pod in namespace monitoring needs to list pods cluster-wide (e.g., Prometheus).
Use a ClusterRole and ClusterRoleBinding:
```
# ClusterRole: cluster-pod-reader
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
```
```
# Create a service account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-sa
  namespace: monitoring
```
```
# ClusterRoleBinding: Bind to the SA
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus-cluster-reader
subjects:
- kind: ServiceAccount
  name: prometheus-sa
  namespace: monitoring
roleRef:
  kind: ClusterRole
  name: cluster-pod-reader
  apiGroup: rbac.authorization.k8s.io
```
### Best practice
- Always follow least privilege: Start restrictive and add verbs as needed.
- Prefer namespace-scoped Roles over ClusterRoles.
- Never use wildcards (*) in production rules.
- For service accounts, avoid the default SA (it has broad permissions in some setups); create dedicated ones.
- Audit regularly: Use kubectl auth can-i --list or tools like kube-bench/RBAC Manager.
- In multi-tenant clusters, combine with NetworkPolicies and quotas.

> Troubleshooting: If access is denied, check with kubectl auth can-i <verb> <resource> --as=<user>.

> ServiceAccounts (SA): Primarily for internal Kubernetes workloads (pods, controllers, operators, etc.) to access the Kubernetes API or other resources.
User and Group subjects in RBAC: Primarily for external human users or integrated identity providers (admins, developers, CI/CD systems, etc.).

## 7. Network policy in K8s
NetworkPolicy is a Kubernetes resource that allows you to define fine-grained network access rules for pods ‚Äî essentially a pod-level firewall.
By default in Kubernetes:
All pods can talk to all other pods (in any namespace) on any port ‚Üí completely open network.
When you apply NetworkPolicy, the default becomes deny-all, and you explicitly allow only the traffic you want.

It answers questions like:
- Which pods can talk to my database?
- Can pods in namespace A reach namespace B?
- Can this pod access the internet?
- Can pods receive traffic only from the ingress controller?

## How NetworkPolicy Works
A NetworkPolicy has three main selectors:
- podSelector ‚Üí Which pods does this policy apply to? (the "targets")
- ingress ‚Üí Who is allowed to send traffic to these pods?
- egress ‚Üí Which destinations these pods are allowed to send traffic to?

### You define allowed sources/destinations using:
- podSelector (same namespace)
- namespaceSelector (other namespaces)
- ipBlock (external CIDRs, e.g., internet access)

### Example: Single NetworkPolicy with Both Ingress and Egress
This policy applies to all pods labeled app: backend in the prod namespace:
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-full-isolation
  namespace: prod
spec:
  podSelector:
    matchLabels:
      app: backend                  # Targets backend pods
  policyTypes:
  - Ingress                       # We are restricting ingress
  - Egress                        # We are restricting egress
  ingress:
  - from:
    - podSelector:                # Allow only from frontend pods (same ns)
        matchLabels:
          app: frontend
    - namespaceSelector:          # OR from monitoring namespace
        matchLabels:
          role: monitoring
    ports:
    - protocol: TCP
      port: 8080                  # Only on app port
  egress:
  - to:
    - podSelector:                # Allow outbound to database pods
        matchLabels:
          app: mysql
    ports:
    - protocol: TCP
      port: 3306
  - to:
    - ipBlock:                    # Allow DNS (UDP to cluster DNS)
        cidr: 10.96.0.10/32      # Example: kube-dns service IP
    ports:
    - protocol: UDP
      port: 53
  - to:
    - ipBlock:                    # Allow HTTPS to internet (e.g., APIs)
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8              # Block internal cluster CIDR
        - 172.16.0.0/12
        - 192.168.0.0/16
    ports:
    - protocol: TCP
      port: 443
```
### What This Achieves
- Ingress: Only frontend pods (same namespace) OR pods from namespaces labeled role: monitoring can reach the backend on port 8080.
- Egress: Backend pods can only:
  - Talk to MySQL on 3306
  - Resolve DNS via kube-dns
  - Make HTTPS calls to the public internet
- All other traffic (in or out) is blocked.

In Kubernetes, multiple NetworkPolicies can apply to the same pod (as long as their podSelector matches the pod's labels).
When this happens, the effective network rules for that pod are the union (OR) of all the allow rules from every matching policy.
- There is no precedence or priority between NetworkPolicies.
- Deny does not override allow ‚Äî if any policy allows a particular connection, the traffic is permitted.
- Only if no policy allows the connection ‚Üí traffic is denied.

> This is a key design principle: NetworkPolicy is allow-list only (whitelist), with an implicit deny-all at the end.

## 8. How do you secure inter-pod communication in AKS using network policies and service mesh (e.g., Istio)?
## 9. What steps would you take to ensure container images are free from known vulnerabilities before they reach production?
## 10. How do you scale AKS workloads based on message queue depth or financial transaction volume?
## 11. Explain how you‚Äôd implement runtime security for containers using tools like Falco or eBPF.
## 12. Explain your production-grade Kubernetes architecture.
## 13. How do you troubleshoot CrashLoopBackOff and ImagePullBackOff?
## 14. What happens when readiness/liveness probes fail?
## 15. How do you implement autoscaling (HPA/VPA/Cluster Autoscaler)?
## 16. How do you manage secrets securely in Kubernetes (Vault / Sealed Secrets)?
## 17. Pods are getting evicted when traffic increases. How do you identify and resolve the eviction reason?
## 18. New pods stay in ‚ÄúPending‚Äù but the cluster autoscaler is not creating new nodes. What could be wrong?
## 19. A pod is running but refuses TCP connections ‚Äì how do you debug?
## 20. How do you handle GPU scheduling in EKS (or GKE)?
## 21. How do you implement secure multi-tenant namespaces in Kubernetes?
## 22. What are the scale limits of kube-proxy iptables, and how do you fix them?
## 23. How do you detect and resolve half-open TCP connections from a service mesh?
## 24. What is the use of Helm in Kubernetes?
## 25. Explain how Kubernetes schedules a pod from request ‚Üí running state.
## 26. What are Deployments, StatefulSets, and DaemonSets?
## 27. What are readiness vs liveness probes and why are they important?
## 28. How do you implement secrets securely in Kubernetes?
## 29. How would you troubleshoot CrashLoopBackOff or ImagePullBackOff?
## 30. Pod in CrashLoopBackOff ‚Äî how do you debug?
## 31. Deployment vs StatefulSet vs DaemonSet ‚Äî real use cases?
## 32. How do you expose a service externally?
33. How do ConfigMaps and Secrets work?
34. RollingUpdate vs Recreate strategy ‚Äî when to use which?

# üê≥ Kubernetes (AKS/EKS) & Container Security
## 1. Write manifest YAML for different kind like service, secrets, volums, pod, deployment, etc.,
## 2. How do you upgrade your k8s cluster?
In a production environment, upgrading a Kubernetes cluster requires careful planning to ensure minimal disruption, high availability, and compatibility.

### First, planning phase:

1. Review the official Kubernetes release notes, changelog, and deprecation notices for the target version to identify breaking changes, removed APIs, or new features.
2. Check for deprecated/removed APIs in your manifests using tools like pluto or kube-no-trouble.
3. Backup etcd (critical for state) and ensure you have recent snapshots.
4. Test the upgrade in a staging environment that mirrors production (same add-ons, workloads, etc.).
5. Verify compatibility of add-ons (e.g., CSI drivers, ingress controllers, monitoring) and third-party tools.

The actual upgrade process depends on whether it's a self-managed cluster (e.g., using kubeadm) or managed (e.g., EKS, GKE, AKS).
### For self-managed (kubeadm) clusters:

1. On the primary control plane node: Run kubeadm upgrade plan to check feasibility and available versions.
2. Upgrade kubeadm: apt update && apt install kubeadm=<target-version>.
3. Apply the upgrade: kubeadm upgrade apply <target-version>.
4. Upgrade kubelet and kubectl on the control plane: apt install kubelet=<target-version> kubectl=<target-version> and restart kubelet.
4. Drain and upgrade additional control plane nodes one by one.
#### For worker nodes: 
- Drain the node (kubectl drain), upgrade kubeadm/kubelet, then kubeadm upgrade node, restart kubelet, and uncordon.
- Post-upgrade: Verify with kubectl get nodes, check pod status, and monitor for issues.

### For managed clusters (common in enterprises):

- AWS EKS: Use the AWS console/CLI (aws eks update-cluster-version) to upgrade the control plane (AWS handles it with zero downtime). Then update managed node groups (via console or eksctl) or self-managed nodes. EKS supports up to 3 minor version skew in newer versions.
- Google GKE: Enable auto-upgrades or manually upgrade via console/gcloud. Options include surge (add nodes during upgrade) or blue-green. GKE handles node auto-repairs and upgrades seamlessly.
- Azure AKS: Use Azure CLI/portal to upgrade control plane, then node pools. Supports planned maintenance windows.

### Best practices I follow:

- Use Pod Disruption Budgets (PDBs) and proper affinities to maintain availability during node drains.
- Upgrade one minor version at a time to reduce risk.
- Monitor during/after with tools like Prometheus or cloud-native monitoring.
- Have a rollback plan (e.g., etcd restore for self-managed; managed services often auto-rollback failed control plane upgrades).
- Schedule during maintenance windows and communicate with stakeholders.

## 6. RBAC
### RBAC: Role Based Access Control
> Securiy machanism that controls who can access the cluster and perform which actions. 
### RBAC:
RBAC is a method of regulating access to resources based on roles of individual users or service accounts. It uses roles and role bndings to define permissions

- Roles: Define set of permissions (verbs like get, list, watch, create, update, patch, delete, etc.) on specifc resources (like pod, deployemnts) with a namespace.
- RoleBindings: Assign those roles to users, groups, or service accounts
- ClusterRoles: Similar to Roles but apply it on cluster level (e.g., list nodes)
- ClusterRoleBindings: Bind ClusterRoles to subjects at the cluster level.
### Example 1: Developer read-only access in a namespace
Suppose we have a namespace dev and want a developer alice@example.com to only view pods and deployments (no create/delete).
```
# Role: pod-reader in namespace dev
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch"]
```
```
# RoleBinding: Bind the role to user alice
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pod-reader-binding
  namespace: dev
subjects:
- kind: User
  name: alice@example.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```
Now, Alice can run kubectl get pods -n dev but not kubectl delete pod.
### Example 2: Service account for a monitoring pod
A pod in namespace monitoring needs to list pods cluster-wide (e.g., Prometheus).
Use a ClusterRole and ClusterRoleBinding:
```
# ClusterRole: cluster-pod-reader
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
```
```
# Create a service account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-sa
  namespace: monitoring
```
```
# ClusterRoleBinding: Bind to the SA
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus-cluster-reader
subjects:
- kind: ServiceAccount
  name: prometheus-sa
  namespace: monitoring
roleRef:
  kind: ClusterRole
  name: cluster-pod-reader
  apiGroup: rbac.authorization.k8s.io
```
### Best practice
- Always follow least privilege: Start restrictive and add verbs as needed.
- Prefer namespace-scoped Roles over ClusterRoles.
- Never use wildcards (*) in production rules.
- For service accounts, avoid the default SA (it has broad permissions in some setups); create dedicated ones.
- Audit regularly: Use kubectl auth can-i --list or tools like kube-bench/RBAC Manager.
- In multi-tenant clusters, combine with NetworkPolicies and quotas.

> Troubleshooting: If access is denied, check with kubectl auth can-i <verb> <resource> --as=<user>.

> ServiceAccounts (SA): Primarily for internal Kubernetes workloads (pods, controllers, operators, etc.) to access the Kubernetes API or other resources.
User and Group subjects in RBAC: Primarily for external human users or integrated identity providers (admins, developers, CI/CD systems, etc.).

## 7. Network policy in K8s
NetworkPolicy is a Kubernetes resource that allows you to define fine-grained network access rules for pods ‚Äî essentially a pod-level firewall.
By default in Kubernetes:
All pods can talk to all other pods (in any namespace) on any port ‚Üí completely open network.
When you apply NetworkPolicy, the default becomes deny-all, and you explicitly allow only the traffic you want.

It answers questions like:
- Which pods can talk to my database?
- Can pods in namespace A reach namespace B?
- Can this pod access the internet?
- Can pods receive traffic only from the ingress controller?

## How NetworkPolicy Works
A NetworkPolicy has three main selectors:
- podSelector ‚Üí Which pods does this policy apply to? (the "targets")
- ingress ‚Üí Who is allowed to send traffic to these pods?
- egress ‚Üí Which destinations these pods are allowed to send traffic to?

### You define allowed sources/destinations using:
- podSelector (same namespace)
- namespaceSelector (other namespaces)
- ipBlock (external CIDRs, e.g., internet access)

### Example: Single NetworkPolicy with Both Ingress and Egress
This policy applies to all pods labeled app: backend in the prod namespace:
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-full-isolation
  namespace: prod
spec:
  podSelector:
    matchLabels:
      app: backend                  # Targets backend pods
  policyTypes:
  - Ingress                       # We are restricting ingress
  - Egress                        # We are restricting egress
  ingress:
  - from:
    - podSelector:                # Allow only from frontend pods (same ns)
        matchLabels:
          app: frontend
    - namespaceSelector:          # OR from monitoring namespace
        matchLabels:
          role: monitoring
    ports:
    - protocol: TCP
      port: 8080                  # Only on app port
  egress:
  - to:
    - podSelector:                # Allow outbound to database pods
        matchLabels:
          app: mysql
    ports:
    - protocol: TCP
      port: 3306
  - to:
    - ipBlock:                    # Allow DNS (UDP to cluster DNS)
        cidr: 10.96.0.10/32      # Example: kube-dns service IP
    ports:
    - protocol: UDP
      port: 53
  - to:
    - ipBlock:                    # Allow HTTPS to internet (e.g., APIs)
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8              # Block internal cluster CIDR
        - 172.16.0.0/12
        - 192.168.0.0/16
    ports:
    - protocol: TCP
      port: 443
```
### What This Achieves
- Ingress: Only frontend pods (same namespace) OR pods from namespaces labeled role: monitoring can reach the backend on port 8080.
- Egress: Backend pods can only:
  - Talk to MySQL on 3306
  - Resolve DNS via kube-dns
  - Make HTTPS calls to the public internet
- All other traffic (in or out) is blocked.

In Kubernetes, multiple NetworkPolicies can apply to the same pod (as long as their podSelector matches the pod's labels).
When this happens, the effective network rules for that pod are the union (OR) of all the allow rules from every matching policy.
- There is no precedence or priority between NetworkPolicies.
- Deny does not override allow ‚Äî if any policy allows a particular connection, the traffic is permitted.
- Only if no policy allows the connection ‚Üí traffic is denied.

> This is a key design principle: NetworkPolicy is allow-list only (whitelist), with an implicit deny-all at the end.

## 7.a. Service Mesh in K8s

## What is a Service Mesh?
Dedicated infrastructure layer for handling service-to-service (east-west) communication in microservices/Kubernetes.
Makes inter-pod traffic secure, observable, reliable, and controllable without changing application code.

## Why Needed?
- Basic K8s: Open network + manual handling of retries, TLS, tracing, routing in app code.
- At scale: Duplicated logic, inconsistent security, poor visibility.
- Mesh moves these concerns out of app code into infrastructure.

## Core Architecture
### Data Plane
- Sidecar proxies (e.g., Envoy in Istio, linkerd-proxy in Linkerd) injected into every pod.
- All inbound/outbound traffic goes through the proxy.
### Control Plane
- Central components that configure all proxies (certificates, policies, telemetry).
- Not in data path.

## Key Features 
### Traffic Management
- Retries, timeouts, circuit breaking
- Load balancing
- Traffic splitting (canary, blue-green)
- Fault injection (chaos testing)

### Security
- Mutual TLS (mTLS) ‚Üí automatic encryption + service identity authentication
- Authorization policies (L7: paths, methods, headers, JWT)
- Identity-based access (using service accounts/SPIFFE)

### Observability
- Distributed tracing (Jaeger/OpenTelemetry)
- Metrics (success rate, latency, request volume)
- Logging + dashboards (Kiali for Istio)


### Popular Meshes
- Istio (Envoy proxy) ‚Üí Most feature-rich, complex, widely used in enterprises.
- Linkerd ‚Üí Lightweight, simple, fast adoption.
- Cilium Service Mesh ‚Üí No sidecar (eBPF-based), high performance.

### How It Works (Quick Flow)
- Install mesh control plane.
- Enable sidecar injection in namespace (istio-injection=enabled).
- Deploy apps ‚Üí pods get proxy automatically.
- Apply declarative policies (PeerAuthentication for mTLS, AuthorizationPolicy for L7 rules).

## 8. How do you secure inter-pod communication in AKS/EKS using network policies and service mesh (e.g., Istio)?
securing inter-pod communication follows a zero-trust model: assume nothing is safe, explicitly allow only required traffic, and add encryption/authentication where possible.

## The two primary mechanisms are:
- Kubernetes Network Policies ‚Äì Built-in L3/L4 (IP + port) firewall.
- Service Mesh (e.g., Istio, Linkerd, Consul) ‚Äì Advanced L7 security with mutual TLS (mTLS), authorization, and observability.

Refer Network policies [here](#7-Network-policy-in-K8s) and Service Mesh [here](#7a-service-mesh-in-k8s).

## 9. What steps would you take to ensure container images are free from known vulnerabilities before they reach production?
Must Know: No
To ensure container images are vulnerability-free before production, integrate security scanning into your CI/CD pipeline. Here's a step-by-step process:

Use Minimal Base Images: Start with official, slim base images (e.g., alpine or distroless) to reduce the attack surface. Avoid bloated images like full Ubuntu.
Scan During Build: In your Dockerfile or CI pipeline (e.g., GitHub Actions, Jenkins), use tools like:
Trivy or Grype: Open-source scanners for OS packages, libraries, and application dependencies. Example command: trivy image --exit-code 1 --no-progress myapp:latest.
Clair or Anchore: Integrate with container registries for deeper analysis.

Sign and Verify Images: Use Cosign or Notary for image signing. In CI, sign images post-build and verify in deployment gates: cosign verify --key cosign.pub myregistry/myapp:latest.
Policy Enforcement: Implement OPA (Open Policy Agent) or admission controllers (e.g., Kyverno) to block deployments of images with high-severity vulnerabilities (CVSS > 7).
Regular Updates and SBOMs: Generate Software Bill of Materials (SBOM) with tools like Syft. Schedule periodic scans and rebuilds for dependencies. Use Image Update Operator in Kubernetes for automated patching.
Registry Scanning: Configure scanners in registries like Harbor or AWS ECR to reject vulnerable pushes.

This multi-layered approach (shift-left security) catches issues early, reducing production risks. Monitor with tools like Trivy Operator in-cluster for runtime checks.
10. How do you scale AKS workloads based on message queue depth or financial transaction volume?
Must Know: No
Azure Kubernetes Service (AKS) supports event-driven and custom metric scaling beyond standard HPA (Horizontal Pod Autoscaler). Use KEDA (Kubernetes Event-Driven Autoscaling) for queue-based scaling and custom metrics for transaction volume.
For Message Queue Depth (e.g., Azure Service Bus or RabbitMQ):

Install KEDA: helm repo add kedacore https://kedacore.github.io/charts && helm install keda kedacore/keda.
Create ScaledObject: Define a YAML with triggers
```
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: queue-scaler
spec:
  scaleTargetRef:
    name: my-deployment
  triggers:
  - type: azure-servicebus
    metadata:
      queueName: orders
      queueLength: "10"  # Scale up at 10 messagesThis scales pods based on queue length.
```
For Financial Transaction Volume (Custom Metrics):

Expose Custom Metrics: Use Azure Monitor or Prometheus Adapter to scrape metrics (e.g., transactions/sec from app logs via Fluentd).
HPA with Custom Metrics:
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: transaction-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: transaction-app
  metrics:
  - type: External
    external:
      metric:
        name: transactions_per_second
      target:
        type: AverageValue
        averageValue: 100
```
Cluster Autoscaler Integration: Enable AKS Cluster Autoscaler to add nodes as pod demand grows.

Monitor with Azure Insights for thresholds. This ensures responsive scaling for bursty workloads like finance, avoiding over-provisioning.
## 11. Explain how you‚Äôd implement runtime security for containers using tools like Falco or eBPF.
Must Know: No
Runtime security monitors containers post-deployment for anomalies. Falco (behavioral detection) and eBPF (kernel tracing) provide deep visibility.
Using Falco:

Install Falco: As a DaemonSet via Helm: helm install falco falcosecurity/falco. It uses syscalls and kernel modules for rules-based alerts.
Define Rules: Edit falco_rules.yaml for custom rules, e.g., detect shell spawns in containers:
```
- rule: shell_in_container
  desc: Spawn shell in container
  condition: spawned_process and container and proc.name = bash
  output: "Shell spawned in container (user=%user.name %proc.cmdline)"
  priority: WARNING
```
Output and Alerting: Forward events to Elasticsearch/Kibana or Slack via Falco's outputs. Integrate with Kubernetes Audit Logs for context.

Using eBPF:

Tools: Cilium (eBPF-based CNI) or Tetragon for tracing. Install Cilium: cilium install.
Implement Tracing: Use eBPF programs to hook kernel events (e.g., network I/O, file access). Example with bpftrace: Trace execve syscalls in pods.
Policy Enforcement: Define eBPF policies in CiliumNetworkPolicies to block unauthorized DNS queries or encrypt traffic.

Combine both: Falco for high-level rules, eBPF for low-overhead tracing. Benefits include zero-overhead monitoring and rapid threat detection (e.g., via Falco's threat feed integration).
## 12. Explain your production-grade Kubernetes architecture.
Must Know: No
A production-grade Kubernetes (K8s) architecture emphasizes high availability (HA), security, observability, and scalability.
Core Components:

Control Plane: HA etcd cluster (3+ nodes), multi-master API servers across AZs. Use managed services like EKS/AKS/GKE for simplicity.
Worker Nodes: Auto-scaling groups in multiple AZs, with taints for dedicated workloads (e.g., GPU nodes). Use node pools for separation (system vs. app).

Networking:

CNI: Calico or Cilium for network policies and encryption (e.g., WireGuard).
Ingress: NGINX or Istio Gateway for external traffic, with TLS termination.

Security:

RBAC & PSP: Role-based access, Pod Security Standards (PSS) enforced via admission controllers.
Secrets: External Vault integration.
Scanning: Gatekeeper/OPA for policy-as-code.

Observability:

Monitoring: Prometheus + Grafana for metrics; ELK stack for logs.
Tracing: Jaeger for distributed tracing.

Storage & Scaling:

CSI: Dynamic provisioning (e.g., EBS for EKS).
Autoscaling: HPA/VPA + Cluster Autoscaler.

Backup/DR:

Velero for etcd/pod backups; multi-cluster federation with Karmada.

This setup supports 99.99% uptime, with CI/CD via ArgoCD for GitOps. Tailor to cloud (e.g., AKS Virtual Nodes for serverless).
## 13. How do you troubleshoot CrashLoopBackOff and ImagePullBackOff?
Must Know: Yes
These are common pod states indicating failures.
CrashLoopBackOff:
Pod restarts repeatedly due to app crashes. Steps:

Describe Pod: kubectl describe pod <pod-name> ‚Äì Check events for errors (e.g., OOMKilled).
View Logs: kubectl logs <pod-name> --previous (for crashed container). Look for stack traces.
Resource Check: Ensure CPU/memory requests/limits aren't exceeded (kubectl top pod).
Exec Debug: If partial startup, kubectl exec -it <pod-name> -- /bin/sh to inspect.
Fix: Adjust init containers, env vars, or app code. Increase restart backoff if needed.

ImagePullBackOff:
Failed to pull image from registry. Steps:

Events: kubectl describe pod ‚Äì Look for "ErrImagePull" (auth/pull quota issues).
ImagePullSecrets: Verify service account has secrets: kubectl get secret. Attach via imagePullSecrets in deployment.
Registry Access: Test pull manually (docker pull). Check quotas (e.g., Docker Hub limits).
Network: Ensure pod CIDR allows registry egress.
Fix: Use private registry mirrors or update image tags.

Use kubectl get events --sort-by='.lastTimestamp' for timelines. Tools like Lens or k9s aid visualization.
## 14. What happens when readiness/liveness probes fail?
Must Know: Yes
Probes ensure pod health; failures trigger actions.

Liveness Probe Failure: Indicates the app is unhealthy (e.g., deadlock). Kubernetes restarts the pod after failureThreshold attempts (default 3). This prevents "zombie" pods but can cause restart loops if misconfigured. Config: HTTP GET /healthz or exec command.
Readiness Probe Failure: Pod isn't ready for traffic (e.g., DB connection pending). Kubernetes removes it from Service endpoints, routing traffic elsewhere. No restart; pod stays running. Use initialDelaySeconds for startup grace.

Importance:

Avoids serving bad traffic (readiness).
Self-heals apps (liveness).
Tune thresholds to match app (e.g., TCP socket probe for simple services).

Example YAML:
YAML
```
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
readinessProbe:
  exec:
    command: ["pg_isready", "-U", "postgres"]
  initialDelaySeconds: 5
```  
Monitor failures via events: kubectl get events | grep Probe.
## 15. How do you implement autoscaling (HPA/VPA/Cluster Autoscaler)?
Must Know: No
Autoscaling optimizes resources dynamically.
Horizontal Pod Autoscaler (HPA):
Scales pod replicas based on metrics.

Install Metrics Server: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml.
Create HPA:
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  scaleTargetRef:
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50Supports custom metrics via Prometheus Adapter.
```
Vertical Pod Autoscaler (VPA):
Adjusts pod resources (CPU/memory) vertically.

Install: kubectl apply -f https://github.com/kubernetes/autoscaler/releases/download/vertical-pod-autoscaler-0.13.0/vpa.yaml.
Modes: Auto (updates limits), Recommend (advisory), Off. Use for non-HPA workloads.

Cluster Autoscaler:
Adds/removes nodes.

In cloud (e.g., AKS): Enable via --expander=random. Label nodes for scale-down.
Config: Set min/max nodes; use annotations like cluster-autoscaler.kubernetes.io/safe-to-evict: false.

Combine: HPA for horizontal, VPA for vertical, CA for cluster. Monitor with kubectl get hpa.
## 16. How do do you manage secrets securely in Kubernetes (Vault / Sealed Secrets)?
Must Know: Yes
Kubernetes Secrets are base64-encoded (not encrypted at rest). For security, use external tools.
HashiCorp Vault:
Dynamic secrets for rotation.

Install Vault injector: Helm chart vault.
Annotate pods: vault.hashicorp.com/role: db in deployment.
Vault Agent injects secrets as env vars/volumes at runtime. Example policy: Dynamic DB creds via database backend.
Benefits: TTL rotation, audit logs.

Sealed Secrets (GitOps-friendly):
Encrypt secrets for Git storage.

Install controller: kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.5/controller.yaml.
Seal: kubeseal < secret.yaml > sealed-secret.yaml (uses asymmetric encryption).
Commit to Git; controller unseals on apply.

Best Practice: Avoid inline secrets; use Vault for prod, Sealed for CI/CD. Enable etcd encryption for at-rest protection.
## 17. Pods are getting evicted when traffic increases. How do you identify and resolve the eviction reason?
Must Know: No
Evictions occur due to node pressure (memory/disk).
Identify:

Events: kubectl get events --field-selector reason=NodePressure or kubectl describe node <node>. Look for "MemoryPressure" or "DiskPressure".
Metrics: kubectl top node for usage; check pod OOM via kubectl describe pod (reason: Evicted).
Kubelet Logs: kubectl logs -n kube-system kubelet-<node>.

Common Reasons:

High traffic exhausts memory (no limits).
Inodes full from logs.

Resolve:

Set Resource Limits: In deployments, add resources: limits: memory: "512Mi". Use VPA for auto-tuning.
Node Affinity: Spread pods with podAntiAffinity.
Eviction Thresholds: Tune kubelet flags (e.g., --eviction-hard=memory.available<100Mi).
Scale Cluster: Trigger Cluster Autoscaler.
Log Rotation: Use sidecar for compression or EFK stack.

Prevent with Pod Disruption Budgets (PDB) to throttle evictions.
## 18. New pods stay in ‚ÄúPending‚Äù but the cluster autoscaler is not creating new nodes. What could be wrong?
Must Know: No
Pending pods indicate scheduling issues; autoscaler inaction points to config problems.
Troubleshooting Steps:

Describe Pod: kubectl describe pod ‚Äì Check for "Insufficient cpu/memory" or taints.
Node Status: kubectl get nodes -o wide and kubectl describe node for capacity. Verify autoscaler logs: kubectl logs -n kube-system cluster-autoscaler.
Common Issues:
Taints/Missing Tolerations: Nodes tainted (e.g., NoSchedule); add tolerations to pod spec.
Node Selectors/Affinity: Pods require labels not on new nodes.
Autoscaler Config: Wrong scale-down disabled or expander misconfigured. Check --nodes=<min:max>.
Quotas: Namespace ResourceQuota blocking. kubectl describe quota.
Cloud Limits: Quota exceeded in AWS/GCP (e.g., instance types).

Test: Scale manually: kubectl scale deployment myapp --replicas=10.

Fix: Align pod specs with node groups; enable scale-from-zero if using KARPENTER.
## 19. A pod is running but refuses TCP connections ‚Äì how do you debug?
Must Know: No
Running pod but connection refusal (RST or timeout) suggests app/network issues.
Debug Steps:

Service Check: kubectl port-forward pod/<pod> 8080:80 ‚Äì Test locally. Verify Service selector matches labels.
App Logs: kubectl logs <pod> for bind errors (e.g., port in use).
Network Policies: kubectl get networkpolicy ‚Äì Ensure allow rules for ingress.
Exec Debug: kubectl exec -it <pod> -- netstat -tlnp (listening ports) or curl localhost:80.
Tools: Deploy netshoot pod: kubectl run netshoot --image=nicolaka/netshoot -it -- /bin/bash, then tcpdump or nslookup.
CNI Issues: Check flannel/calico pods; kubectl logs -n kube-system calico-node.
Firewall/SELinux: On nodes, verify iptables.

Root Causes: Wrong port, health probe blocking, or MTU mismatch. Use kubectl exec with nc -zv <service> 80 for connectivity.
## 20. How do you handle GPU scheduling in EKS (or GKE)?
Must Know: No
GPU scheduling requires node labeling and extended resources.
In EKS (AWS):

Node Group: Launch p3/g4dn instances; label nodes: kubectl label nodes <node> gpu=true.
Device Plugin: Install NVIDIA plugin: kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/master/nvidia-device-plugin.yml.
Pod Spec: Request GPUs:
```
resources:
  limits:
    nvidia.com/gpu: 1
nodeSelector:
  gpu: "true"
```
Scheduler: Use default or Volcano for advanced queuing.

In GKE:
Similar; use preemptible GPUs with accelerator: type=nvidia-tesla-k80,count=1 in node pool. Enable NVIDIA GPU Operator for automation.
Monitor with DCGM exporter. Handle sharing via time-slicing in plugin config. Scale with Cluster Autoscaler on GPU pools.
## 21. How do you implement secure multi-tenant namespaces in Kubernetes?
Must Know: No
Multi-tenancy isolates teams/workloads.

Namespaces: Create per tenant: kubectl create ns tenant-a.
RBAC: Namespace-scoped roles:
```
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: tenant-a
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
```
Bind to users/groups via RoleBinding. Use cert-manager for client certs.
Network Policies: Isolate traffic:
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
spec:
  podSelector: {}
  ingress: []  # Deny all by default
  policyTypes: [Ingress]
```
  Allow specific labels.
Resource Quotas: kubectl create quota tenant-quota --hard=cpu=4,memory=8Gi -n tenant-a.
Pod Security: Enforce PSS (privileged=false) via admission. Use Kyverno for policies.

Audit with Falco. For advanced, use Loft or KubeVirt for VM isolation.
## 22. What are the scale limits of kube-proxy iptables, and how do you fix them?
Must Know: No
Kube-proxy in iptables mode (default) has limits: ~1000 pods/node due to iptables chain explosion (O(n^2) rules).

Symptoms: High CPU on kube-proxy, latency spikes. Check: kubectl logs -n kube-system kube-proxy-<pod>.

Fixes:

Switch to IPVS: More scalable (O(n) hashing). Enable: Add --proxy-mode=ipvs to kube-proxy args in manifest. Requires kernel modules (ip_vs).YAML
```
args:
- --proxy-mode=ipvs
- --ipvs-scheduler=rr
```
Node Sizing: Limit pods/node via --max-pods kubelet flag.
Service Mesh: Offload to Istio/Linkerd for L7 routing.
Monitor: Use conntrack -C for connection table size; tune sysctl net.netfilter.nf_conntrack_max.

IPVS supports up to 10k+ services efficiently.
## 23. How do you detect and resolve half-open TCP connections from a service mesh?
Must Know: No
Half-open connections (SYN sent, no ACK) indicate drops/timeouts in meshes like Istio.
Detection:

Telemetry: In Istio, enable access logs: pilot envoyfilter for metrics. Query Prometheus: sum(rate(istio_requests_total{response_code=~"5xx"}[5m])).
Tools: ss -tan | grep SYN-RECV on nodes; Kiali dashboard for connection pools.
Traces: Jaeger shows latency spikes.

Resolution:

Timeouts: Tune Envoy filters:
```
httpFilters:
- name: envoy.filters.http.router
  typed_config:
    "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
timeout: 30s  # Upstream timeout
```
Circuit Breakers: Set max connections: outlierDetection.max_ejection_percent: 50.
Health Checks: Active probes to evict unhealthy upstreams.
CNI Tuning: Increase conntrack limits; use DSR (Direct Server Return) mode.

Root causes: Overloaded pods or network flaps. Resolve by scaling or retry policies.
## 24. What is the use of Helm in Kubernetes?
Must Know: Yes
Helm is a package manager for Kubernetes, simplifying app deployment like apt for Linux.

Charts: Templated YAML bundles (e.g., nginx chart with values.yaml for customization).
Uses:
Install: helm install my-nginx stable/nginx.
Upgrade: helm upgrade my-nginx stable/nginx --set replicaCount=3.
Rollback: helm rollback my-nginx 1.
Repository: Add charts from Artifact Hub.


Benefits: Versioning, dependency management, GitOps integration (with Flux). Avoids manual YAML sprawl for complex apps.
## 25. Explain how Kubernetes schedules a pod from request ‚Üí running state.
Must Know: Yes
Pod lifecycle: Create ‚Üí Pending ‚Üí Running.

API Server: kubectl apply validates and stores Pod object in etcd.
Scheduler: Watches API; filters nodes (capacity, affinity, taints). Scores (e.g., spread evenly) and binds Pod to node via .spec.nodeName.
Kubelet: On selected node, watches bound Pods; pulls image (container runtime like containerd).
Container Runtime: Starts containers; reports status back to API (via Kubelet). Pod becomes Running if probes pass.

Fails: Pending (unschedulable). Tune scheduler with profiles or descheduler cronjobs.
## 26. What are Deployments, StatefulSets, and DaemonSets?
Must Know: Yes

Deployments: For stateless apps (e.g., web servers). Manages ReplicaSets for rolling updates, scaling. Example: kubectl create deployment nginx --image=nginx.
StatefulSets: For stateful apps (e.g., databases). Provides stable identity (pod names like db-0), ordered deployment/scaling, persistent storage. Use for MySQL clusters.
DaemonSets: Runs one pod per node (e.g., logging agents like Fluentd). Ensures coverage; updates roll across nodes.

Choose: Deployment for APIs, StatefulSet for ordered services, DaemonSet for node-local tasks.
## 27. What are readiness vs liveness probes and why are they important?
Must Know: Yes

Readiness Probe: Checks if pod can handle traffic (e.g., HTTP 200 on /ready). Failure: Excludes from Service load balancing. Prevents sending requests to starting pods.
Liveness Probe: Checks if pod is alive (e.g., exec curl /health). Failure: Restarts pod. Catches crashes/deadlocks.

Importance:

Ensures reliable traffic routing (readiness).
Enables self-healing (liveness).
Use startupProbe for slow boots. Misconfig leads to flapping; tune delays/thresholds.

Example: Readiness for DB connectivity, liveness for app heartbeat.
## 28. How do you implement secrets securely in Kubernetes?
Must Know: Yes
Secrets store sensitive data (e.g., API keys).

Create: kubectl create secret generic db-pass --from-literal=password=secret (base64 in etcd).
Use: Mount as volume/env:
```
volumes:
- name: secret-volume
  secret:
    secretName: db-pass
containers:
- volumeMounts:
  - name: secret-volume
    mountPath: /etc/secrets
```
Secure: Encrypt etcd; avoid Git commits. Use Vault/CSI driver for dynamic injection. Rotate via operators.

Limitations: Not encrypted in transit to pods. Prefer external managers.
## 29. How would you troubleshoot CrashLoopBackOff or ImagePullBackOff?
Must Know: Yes
(See Question 13 for detailed steps.)
Summary:

CrashLoopBackOff: Logs (kubectl logs --previous), describe (events), resources, exec.
ImagePullBackOff: Secrets, registry access, describe events.
Use kubectl debug for ephemeral containers in newer K8s.

## 30. Pod in CrashLoopBackOff ‚Äî how do you debug?
Must Know: Yes

Events/Describe: kubectl describe pod <name> ‚Äì Exit codes, OOM?
Logs: kubectl logs <pod> -c <container> --tail=50 --previous.
Resource Usage: kubectl top pod during crash.
Debug Pod: kubectl debug <pod> -it --image=busybox --share-processes (attach to namespaces).
Probes: Disable temporarily to isolate.
Init Containers: Check sequential failures.

Fix: Limits, deps, or app fixes. Monitor with events tailing.
## 31. Deployment vs StatefulSet vs DaemonSet ‚Äî real use cases?
Must Know: Yes
(See Question 26 for definitions.)

Deployment: Stateless web app (e.g., Nginx replicas for load balancing; easy scaling).
StatefulSet: Distributed DB (e.g., Cassandra; needs stable hostnames like cass-0 for peer discovery).
DaemonSet: Monitoring agent (e.g., Node Exporter on every node for metrics collection).

Use Deployment for 80% apps; others for specific needs.
32. How do you expose a service externally?
Must Know: Yes

Service Types:
ClusterIP (internal): Default.
NodePort: Expose on node IP:port (30000‚Äì32767). type: NodePort.
LoadBalancer: Cloud LB (e.g., AWS ELB). type: LoadBalancer.

Ingress: L7 routing: Install NGINX Ingress Controller, then:
```
apiVersion: networking.k8s.io/v1
kind: Ingress
spec:
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: my-service
            port:
              number: 80Add TLS with secrets.
```
Use Ingress for prod; NodePort for dev.
## 33. How do ConfigMaps and Secrets work?
Must Know: Yes

ConfigMaps: Non-sensitive config (e.g., env vars, files). Create: kubectl create configmap app-config --from-literal=debug=true. Mount: As volume or envFrom. Updates trigger pod restart (via ownerRef).
Secrets: Sensitive data (base64-encoded). Similar: kubectl create secret generic tls --from-file=tls.key=server.key.

Both: Projected volumes for hot-reload (subPath). Use for decoupling config from code; external tools for secrets.
## 34. RollingUpdate vs Recreate strategy ‚Äî when to use which?
Must Know: Yes
In Deployments: strategy: type: RollingUpdate (default).

RollingUpdate: Gradually replaces pods (maxUnavailable/maxSurge). Zero-downtime for stateless apps. Params: maxUnavailable: 25%. Use for web services.
Recreate: Kills all pods, then starts new. Downtime; for apps needing all-or-nothing (e.g., DB migrations with shared state).

Choose Rolling for HA; Recreate for init-heavy or incompatible updates.7.8s
